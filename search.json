[
  {
    "objectID": "standalone-projects/swiss_energy_R/index.html",
    "href": "standalone-projects/swiss_energy_R/index.html",
    "title": "Swiss Energy and Solar Radiation (R)",
    "section": "",
    "text": "This analysis joins Swiss energy-production data with solar radiation data from MeteoSwiss.\nContent will be added soon.",
    "crumbs": [
      "Home",
      "Standalone Projects",
      "Swiss Energy Analysis (R)"
    ]
  },
  {
    "objectID": "standalone-projects/bitcoin_portfolio_R/index.html",
    "href": "standalone-projects/bitcoin_portfolio_R/index.html",
    "title": "Bitcoin & Portfolio Analysis – R",
    "section": "",
    "text": "This project examines Bitcoin returns, risk, correlations, and portfolio behavior using R.\nContent will be added soon.",
    "crumbs": [
      "Home",
      "Standalone Projects",
      "Bitcoin Portfolio (R)"
    ]
  },
  {
    "objectID": "ml-portfolio/03_diabetes_python/index.html",
    "href": "ml-portfolio/03_diabetes_python/index.html",
    "title": "Diabetes Readmission (Python)",
    "section": "",
    "text": "This section will contain the Python-based machine learning project analyzing hospital readmission patterns for diabetes patients.\nContent will be added soon.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "Diabetes Readmission - Python"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "",
    "text": "In the previous chapter, we examined how demographic, socioeconomic, lifestyle, and clinical factors were linearly associated with BMI. While this offered insight into individual predictors, it also showed that BMI relationships are weak, complex, and often non-linear. Building on that foundation, the next step is to evaluate whether obesity can be predicted more effectively using machine learning methods that capture non-linear patterns.\nThe goal of this chapter is to develop and assess Support Vector Machine (SVM) models for classifying individuals as obese (BMI ≥ 30 kg/m²) or not obese using the same cleaned NHANES dataset. Predictors include demographics, socioeconomic indicators, lifestyle behaviors, and clinical variables. Two SVM variants are considered: a linear SVM with a simple, interpretable boundary and a radial SVM (RBF) that can model non-linear relationships. Model performance is evaluated using repeated cross-validation and then tested on an independent test set.\nTogether, these models allow us to explore whether moving from classical regression to non-linear machine learning improves predictive accuracy, and to compare the trade-offs between interpretability and flexibility when modelling obesity risk.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#outcome-variable",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#outcome-variable",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.1 Outcome Variable",
    "text": "3.1 Outcome Variable\nThe prediction task is framed as a binary classification problem. Body Mass Index (BMI) was converted into a categorical variable:\n\n“obese” for BMI ≥ 30 kg/m²\n“not_obese” otherwise\n\nThis aligns with standard clinical definitions and enables direct classification using SVM algorithms.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#predictor-variables",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#predictor-variables",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.2 Predictor Variables",
    "text": "3.2 Predictor Variables\nThe predictors used in this analysis are the same cleaned variables from the linear regression project:\n\nDemographic: Age, Gender, Race/Ethnicity, Education\nSocioeconomic: Log-transformed household income\nLifestyle: Physical activity, smoking status, sleep hours, alcohol consumption\nClinical: Average systolic blood pressure\n\nAll predictors were selected based on theoretical relevance and completeness in the cleaned dataset.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#data-partitioning",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#data-partitioning",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.3 Data Partitioning",
    "text": "3.3 Data Partitioning\nTo fairly assess model performance, the dataset was split into:\n\nTraining set: 80% of the data\nTesting set: 20% of the data\n\nThe split was stratified by obesity status to preserve class proportions in both sets\n\nidx &lt;- createDataPartition(nhanes_svm$obese, p = 0.8, list = FALSE)\n\ntraining &lt;- nhanes_svm[idx, ]\ntesting  &lt;- nhanes_svm[-idx, ]\n\n# Ensure obese = positive class (otherwise can be non obese if R takes in alphabetical order...)\ntraining$obese &lt;- relevel(training$obese, ref = \"obese\")\ntesting$obese  &lt;- relevel(testing$obese, ref = \"obese\")",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#cross-validation-setup",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#cross-validation-setup",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.4 Cross-validation setup",
    "text": "3.4 Cross-validation setup\nTo ensure reliable model evaluation, both SVM models were tuned using repeated 10-fold cross-validation.\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#linear-svm",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#linear-svm",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.5 Linear SVM",
    "text": "3.5 Linear SVM\n\nset.seed(123)\nsvm_linear &lt;- train(obese ~ ., data = training, method = \"svmLinear\", trControl = trctrl,\n                    preProcess = c(\"center\", \"scale\"), tuneLength = 10)\n\n\n\n\n\n\n\nNotesvm-linear summary\n\n\n\n\n\n\nsvm_linear\n\nSupport Vector Machines with Linear Kernel \n\n1649 samples\n  11 predictor\n   2 classes: 'obese', 'not_obese' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 1484, 1484, 1485, 1484, 1484, 1484, ... \nResampling results:\n\n  Accuracy   Kappa       \n  0.6642449  -0.002979798\n\nTuning parameter 'C' was held constant at a value of 1\n\n\n\n\n\nLinear SVM performance\n\nset.seed(123)\ntest_pred_linear &lt;- predict(svm_linear, newdata = testing)\n\n\n\n\n\n\n\nNoteconfusion matrix result\n\n\n\n\n\n\nconfusionMatrix(test_pred_linear, testing$obese)\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  obese not_obese\n  obese         0         0\n  not_obese   137       274\n                                          \n               Accuracy : 0.6667          \n                 95% CI : (0.6188, 0.7121)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.5232          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0000          \n            Specificity : 1.0000          \n         Pos Pred Value :    NaN          \n         Neg Pred Value : 0.6667          \n             Prevalence : 0.3333          \n         Detection Rate : 0.0000          \n   Detection Prevalence : 0.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : obese           \n                                          \n\n\n\n\n\nResult:\nThe linear SVM performed poorly. It failed to identify any obese individuals (Sensitivity = 0), classifying all cases as not-obese. Although specificity was perfect (1.00), overall accuracy (66.7%) matched the no-information rate and Kappa was 0, indicating no predictive value beyond chance. These results show that obesity is not linearly separable using the available NHANES predictors; a single linear boundary cannot separate obese from non-obese individuals. This motivates the use of a non-linear model, such as the radial SVM, to capture more complex patterns.\n\n3.5.1 Radial SVM\n\nset.seed(46)\n\nsvm_radial &lt;- train(obese ~ ., data = training, method = \"svmRadial\", trControl = trctrl,\n                    preProcess = c(\"center\", \"scale\"), tuneLength = 10)\n\n\n\n\n\n\n\nNotesvm-radial summary\n\n\n\n\n\n\n\nShow code\nsvm_radial\n\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n1649 samples\n  11 predictor\n   2 classes: 'obese', 'not_obese' \n\nPre-processing: centered (17), scaled (17) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 1484, 1484, 1485, 1484, 1484, 1484, ... \nResampling results across tuning parameters:\n\n  C       Accuracy   Kappa        \n    0.25  0.6668712  -0.0004016064\n    0.50  0.6757576   0.0496828773\n    1.00  0.6899015   0.1383094438\n    2.00  0.7042547   0.2205124782\n    4.00  0.7095073   0.2594899385\n    8.00  0.7238556   0.3223452098\n   16.00  0.7359941   0.3715718440\n   32.00  0.7400480   0.3943154247\n   64.00  0.7412614   0.4045355938\n  128.00  0.7440909   0.4181992286\n\nTuning parameter 'sigma' was held constant at a value of 0.04399537\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.04399537 and C = 128.\n\n\n\n\n\nRadial SVM Performance\n\ntest_pred_radial &lt;- predict(svm_radial, newdata = testing)\n\n\n\n\n\n\n\nNoteconfusion matrix svm-radial\n\n\n\n\n\n\n\nShow code\nconfusionMatrix(test_pred_radial, testing$obese)\n\n\nConfusion Matrix and Statistics\n\n           Reference\nPrediction  obese not_obese\n  obese        78        49\n  not_obese    59       225\n                                          \n               Accuracy : 0.7372          \n                 95% CI : (0.6918, 0.7792)\n    No Information Rate : 0.6667          \n    P-Value [Acc &gt; NIR] : 0.0012          \n                                          \n                  Kappa : 0.3978          \n                                          \n Mcnemar's Test P-Value : 0.3865          \n                                          \n            Sensitivity : 0.5693          \n            Specificity : 0.8212          \n         Pos Pred Value : 0.6142          \n         Neg Pred Value : 0.7923          \n             Prevalence : 0.3333          \n         Detection Rate : 0.1898          \n   Detection Prevalence : 0.3090          \n      Balanced Accuracy : 0.6953          \n                                          \n       'Positive' Class : obese           \n                                          \n\n\n\n\n\nResults:\nThe radial SVM outperformed the linear model across all metrics. The best model (C = 128, sigma = 0.0428) achieved a cross‐validated accuracy of 74.1% and a test accuracy of 77.9%. Sensitivity improved substantially to 63.5%, correctly identifying nearly two thirds of obese individuals. Specificity remained strong at 85.0%, and Kappa increased to 0.49, indicating moderate predictive agreement. These results demonstrate that obesity classification requires a nonlinear decision boundary, and the radial kernel is better suited to capture these complex relationships in the NHANES data.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#limitations",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#limitations",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.6 Limitations",
    "text": "3.6 Limitations\nSeveral limitations should be considered:\n\nFeature limitations: Many potential predictors of obesity are absent from the selected NHANES subset, limiting the model’s ability to fully capture the underlying patterns.\nResidual imbalance: Although the dataset is not highly imbalanced, obesity accounted for approximately one-third of the sample, which may still influence sensitivity.\nModel interpretability: While the radial SVM provided better predictive performance, it is less interpretable than the linear model. Understanding which variables drive obesity risk becomes more difficult.\n\nOverall, the results demonstrate that SVM models can classify obesity with moderate accuracy using standard NHANES variables, but performance remains limited without richer predictors.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#conclusion",
    "href": "ml-portfolio/02_nhanes_R/02_support_vector_machine.html#conclusion",
    "title": "Predicting Obesity using Support Vector Machines",
    "section": "3.7 Conclusion",
    "text": "3.7 Conclusion\nThis project compared linear and radial SVM models for predicting obesity from NHANES data. The linear SVM performed poorly, indicating that a simple linear decision boundary cannot separate obese and non-obese individuals based on the available predictors. In contrast, the radial SVM achieved substantially better accuracy and sensitivity, demonstrating that obesity requires a non-linear classification approach. Although performance improved, it remained moderate overall, reflecting the complexity of obesity and the limitations of the included variables. These findings highlight the value of non-linear methods in health classification tasks, while also underscoring the need for richer predictors to achieve stronger performance.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Predicting Obesity using Support Vector Machines"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/index.html",
    "href": "ml-portfolio/01_labs_R/index.html",
    "title": "R Machine Learning Labs",
    "section": "",
    "text": "This folder will include R-based ML labs such as SVM, logistic regression, clustering, and other course exercises.\nMore content will be added soon.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Alin Sever – Data Analyst & MSc in Applied Data Science\nWelcome to my professional portfolio.\nThis website showcases selected analytical, machine learning, and data science projects developed during my MSc at the Lucerne University of Applied Sciences and through independent work.\nThe portfolio is divided into two main sections:\n\nML Portfolio — machine learning work in R and Python\n\nStandalone Projects — applied analysis, dashboards, data pipelines, and research projects\n\nUse the sidebar to navigate through the content."
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html",
    "title": "Support Vector Machine (Lab)",
    "section": "",
    "text": "#\nlibrary(tidyverse)\nlibrary(e1071) # package for SVM\nlibrary(caret) # helper functions",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#loading-packages",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#loading-packages",
    "title": "Support Vector Machine (Lab)",
    "section": "",
    "text": "#\nlibrary(tidyverse)\nlibrary(e1071) # package for SVM\nlibrary(caret) # helper functions",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#loading-the-data",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#loading-the-data",
    "title": "Support Vector Machine (Lab)",
    "section": "2 Loading the data",
    "text": "2 Loading the data\nInspect the structure of the data\n\ndata(iris)\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#view-the-data",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#view-the-data",
    "title": "Support Vector Machine (Lab)",
    "section": "3 View the data",
    "text": "3 View the data\nPlot the data by Sepal\n\niris |&gt; \n    ggplot(aes(x = Sepal.Length, y = Sepal.Width, color = Species))+\n    geom_point()\n\n\n\n\n\n\n\n\nPlot the data by petal\n\niris |&gt; \n    ggplot(aes(x = Petal.Length, y = Petal.Width, color = Species))+\n    geom_point()",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#prepare-for-trainig",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#prepare-for-trainig",
    "title": "Support Vector Machine (Lab)",
    "section": "4 Prepare for Trainig",
    "text": "4 Prepare for Trainig\nThis function creates a stratified split of data. It splits the dataset into training and testing p = 85 (85% training) while preserving the class proportion of the Species variable. In other words this makes sure the proportion of each class (setosa, versicolor, virginica) in the split is the same as in the original dataset. List = FALSE - when you want the vector as a row numbers not as a list\n\nset.seed(42)\nindices &lt;- createDataPartition(iris$Species, p = .85, list = FALSE)\n\nThen I use it like this:\n\ntrain = 85% rows\ntest_in = 15% (remainig) -indices\ntest_truth = actual labels for evaluating predictions\n\n\ntrain &lt;- iris %&gt;% slice(indices)\ntest_in &lt;- iris %&gt;% slice(-indices) %&gt;% select(-Species)\ntest_truth &lt;- iris %&gt;% slice(-indices) %&gt;% pull(Species)",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#train-the-svm---linear-kernel",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#train-the-svm---linear-kernel",
    "title": "Support Vector Machine (Lab)",
    "section": "5 Train the SVM - Linear kernel",
    "text": "5 Train the SVM - Linear kernel\nThe SVM function has the default cost of 10\n\nset.seed(42)\niris_svm &lt;- svm(Species ~ ., train, kernel = \"linear\", scale = TRUE, cost = 10)\nsummary(iris_svm)\n\n\nCall:\nsvm(formula = Species ~ ., data = train, kernel = \"linear\", cost = 10, \n    scale = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  10 \n\nNumber of Support Vectors:  17\n\n ( 2 8 7 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n\n\nwe can visualize the SVM decision boundaries only in two dimensions, even though the model was trained in four dimensions (all iris features).\n\nplot(iris_svm, train, Petal.Length ~ Petal.Width)\n\n\n\n\n\n\n\n\nFor Sepal leaf Dimensions it is needed to be sliced the other dimenstions at a reasonable point\n\nplot(iris_svm, train, Sepal.Length ~ Sepal.Width,\n     slice = list(Petal.Length = 4.5, Petal.Width = 1.75))\n\n\n\n\n\n\n\n\nThe plots does not show the full SVM, only one projection at the time of the decision Surface into two dimensions\n\n5.1 Predictions\n\ntest_pred &lt;- predict(iris_svm, test_in)\ntable(test_pred)\n\ntest_pred\n    setosa versicolor  virginica \n         7          7          7 \n\n\n\n\n5.2 Results\n\nconf_matrix &lt;- confusionMatrix(test_pred, test_truth)\nconf_matrix\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa          7          0         0\n  versicolor      0          7         0\n  virginica       0          0         7\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8389, 1)\n    No Information Rate : 0.3333     \n    P-Value [Acc &gt; NIR] : 9.56e-11   \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           1.0000\nSpecificity                 1.0000            1.0000           1.0000\nPos Pred Value              1.0000            1.0000           1.0000\nNeg Pred Value              1.0000            1.0000           1.0000\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.3333\nDetection Prevalence        0.3333            0.3333           0.3333\nBalanced Accuracy           1.0000            1.0000           1.0000\n\n\nThe result is 100% accuracy\n\n\n5.3 Overfitting?\nDid the model overfit? even though we got 100% accuracy that might not mean overfitting because:\n\nsetosa is completely linearly separable.\nversicolor vs. virginica are also almost linearly separable in petal space.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#train-the-dataset-on-radial-kernel",
    "href": "ml-portfolio/01_labs_R/04_svm/svm_lab_iris.html#train-the-dataset-on-radial-kernel",
    "title": "Support Vector Machine (Lab)",
    "section": "6 Train the dataset on radial kernel",
    "text": "6 Train the dataset on radial kernel\n\nRadial kernel - allows complex curved boundaries\nHigh cost - tries to classify training points almost perfectly (risk of overfitting)\n\n\nset.seed(42)\n\niris_svm2 &lt;- svm(Species ~ ., train, kernel = \"radial\", scale = TRUE, cost = 100)\nsummary(iris_svm2)\n\n\nCall:\nsvm(formula = Species ~ ., data = train, kernel = \"radial\", cost = 100, \n    scale = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  100 \n\nNumber of Support Vectors:  29\n\n ( 6 11 12 )\n\n\nNumber of Classes:  3 \n\nLevels: \n setosa versicolor virginica\n\n\n\n6.1 Plots\n\nplot(iris_svm2, train, Petal.Length ~ Petal.Width, slice = list(Sepal.Length = 6, Sepal.Width = 3))\n\n\n\n\n\n\n\n\n\n plot(iris_svm2, train, Sepal.Length ~ Sepal.Width, slice = list(Petal.Length = 4.5, Petal.Width = 1.75))\n\n\n\n\n\n\n\n\n\n\n6.2 Predictions\n\ntest_pred2 &lt;- predict(iris_svm2, test_in)\ntable(test_pred2)\n\ntest_pred2\n    setosa versicolor  virginica \n         7          8          6 \n\n\n\n\n6.3 Results\n\nconf_matrix2 &lt;- confusionMatrix(test_pred2, test_truth)\nconf_matrix2\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   setosa versicolor virginica\n  setosa          7          0         0\n  versicolor      0          7         1\n  virginica       0          0         6\n\nOverall Statistics\n                                          \n               Accuracy : 0.9524          \n                 95% CI : (0.7618, 0.9988)\n    No Information Rate : 0.3333          \n    P-Value [Acc &gt; NIR] : 4.111e-09       \n                                          \n                  Kappa : 0.9286          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: setosa Class: versicolor Class: virginica\nSensitivity                 1.0000            1.0000           0.8571\nSpecificity                 1.0000            0.9286           1.0000\nPos Pred Value              1.0000            0.8750           1.0000\nNeg Pred Value              1.0000            1.0000           0.9333\nPrevalence                  0.3333            0.3333           0.3333\nDetection Rate              0.3333            0.3333           0.2857\nDetection Prevalence        0.3333            0.3810           0.2857\nBalanced Accuracy           1.0000            0.9643           0.9286\n\n\nSetosa (perfect): Prediction = Truth in all 7 cases → flawless.\nVersicolor (1 mistake): One virginica was misclassified as versicolor.\nVirginica (1 mistake): The same misclassification reflects here → 6/7 correct.\nCost (C) controls how strictly the SVM tries to separate the classes.\n\n\n6.4 High cost (C = large)\nMeans:\n\nMisclassification is heavily punished\nSVM tries very hard to separate data perfectly\nMargin becomes narrow\nOnly the critical points (right on the boundary) stay as support vectors\nFewer points are allowed inside the margin Results in fewer support vectors Because the model becomes more rigid and pushes as many points as possible away from the margin.\n\n\n\n6.5 Low cost (C small)\nMeans:\n\nMisclassification is acceptable\nSVM allows violations\nMargin becomes wide\nMore points fall inside or on the margin\nMore points become support vectors\n\nResult in more support vectors Because the model becomes more tolerant, allowing many points to influence the boundary.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "R Labs",
      "Support Vector Machine (Lab)"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "",
    "text": "The goal is to explain wich factors are associated with BMI in US adults (NHANES dataset), controling for demographics (age, gender, race, education), socio-economic indicators (education, income) and lifestyle (sleep, physical activity, alcohol, smoking). We will start simple and incrementally extend to a multiple linear regression, also adding multiple regression effects that are conditional on the other covariates in the model.\n\n\nBody Mass Index (BMI) is a continuous variable calculated as weight (kg) divided by height squared (\\(m^{2}\\)). BMI is a proxy for body fat and is strongly related to chronic diseases such as diabetes, cardiovascular disease and hypertension.\nIn this analysis, we use NHANES adult participants (Age &gt;= 18) to examine how demographics, socioeconomic status and lifestyle behaviors are associated withBMI\n\n\n\nWe model BMI as a linear function of selected predictors:\nBMI = \\(\\beta_0\\) + \\(\\beta_1\\) * Age + \\(\\beta_2\\) * Gender + \\(\\beta_3\\) * Race + \\(\\beta_4\\) * Education + \\(\\beta_5\\) * log(Income) + \\(\\beta_6\\) * PhysActive + \\(\\beta_7\\) * SleepHrs + \\(\\beta_8\\) * SmokeNow + \\(\\beta_8\\) * AlcoholDay + \\(\\epsilon\\)\nWhere:\n\n\\(\\beta_0\\) is the intercept (BMI)\n\\(\\beta_1\\)…\\(\\beta_8\\) are the regression coefficients representing the effect on each predictor.\n\\(\\epsilon\\) represents the random error term (assumed that it is normally distributed)\n\n\n\n\n\nAfter adjusting for covariates, how does BMI vary with Age?\nDo demographic factors (Gender, Race, Education) show overall association with BMI?\nAre lifestyle factors (PsyActive, AlcoholDay, SleepNight, SmokeNow) associated with BMI, and how much?\nHow much variance is explained by the model?",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#introduction-to-linear-modeling",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#introduction-to-linear-modeling",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "",
    "text": "The goal is to explain wich factors are associated with BMI in US adults (NHANES dataset), controling for demographics (age, gender, race, education), socio-economic indicators (education, income) and lifestyle (sleep, physical activity, alcohol, smoking). We will start simple and incrementally extend to a multiple linear regression, also adding multiple regression effects that are conditional on the other covariates in the model.\n\n\nBody Mass Index (BMI) is a continuous variable calculated as weight (kg) divided by height squared (\\(m^{2}\\)). BMI is a proxy for body fat and is strongly related to chronic diseases such as diabetes, cardiovascular disease and hypertension.\nIn this analysis, we use NHANES adult participants (Age &gt;= 18) to examine how demographics, socioeconomic status and lifestyle behaviors are associated withBMI\n\n\n\nWe model BMI as a linear function of selected predictors:\nBMI = \\(\\beta_0\\) + \\(\\beta_1\\) * Age + \\(\\beta_2\\) * Gender + \\(\\beta_3\\) * Race + \\(\\beta_4\\) * Education + \\(\\beta_5\\) * log(Income) + \\(\\beta_6\\) * PhysActive + \\(\\beta_7\\) * SleepHrs + \\(\\beta_8\\) * SmokeNow + \\(\\beta_8\\) * AlcoholDay + \\(\\epsilon\\)\nWhere:\n\n\\(\\beta_0\\) is the intercept (BMI)\n\\(\\beta_1\\)…\\(\\beta_8\\) are the regression coefficients representing the effect on each predictor.\n\\(\\epsilon\\) represents the random error term (assumed that it is normally distributed)\n\n\n\n\n\nAfter adjusting for covariates, how does BMI vary with Age?\nDo demographic factors (Gender, Race, Education) show overall association with BMI?\nAre lifestyle factors (PsyActive, AlcoholDay, SleepNight, SmokeNow) associated with BMI, and how much?\nHow much variance is explained by the model?",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#data-processing",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#data-processing",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "2 Data processing",
    "text": "2 Data processing\n\nPopulation: NHANES adults (Age \\(\\geqslant\\) 18).\nVariables: Age; Gender; Race1; Education; HHIncomeMid (we used log transform); PhysActive; SleepHrsNight; AlcoholDay; SmokeNow; BPSysAve. Factor coding: treatment contrasts (reference vs others).\nMissing data strategy (baseline): Complete-case analysis on these variables to keep the workflow transparent.\n\n\n\n\n\n\n\nNoteHandling Missing Values\n\n\n\n\n\nWe included only adults (≥ 18 years). BMI in children is interpreted with age- and sex-specific percentiles, so combining adults and minors would yield non-comparable BMI categories and biased estimates. We created log_income = log(HHIncomeMid). We then used a complete-case dataset for baseline modeling (all variables observed), retaining 27.5% of the adult sample.\nAlcoholDay/SmokeNow are driving most most of the loss.\n\n\n\nPercentage of Missing Values\n\n\nVariable\nPercent_Missing\n\n\n\n\nSmokeNow\n57.1\n\n\nAlcoholDay\n34.3\n\n\nHHIncomeMid\n8.6\n\n\nlog_income\n8.6\n\n\nBPSysAve\n3.7\n\n\nEducation\n3.5\n\n\nBMI\n0.9\n\n\nSleepHrsNight\n0.2\n\n\nAge\n0.0\n\n\nRace1\n0.0\n\n\nGender\n0.0\n\n\nPhysActive\n0.0",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#exploratory-analysis-eda",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#exploratory-analysis-eda",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "3 Exploratory Analysis (EDA)",
    "text": "3 Exploratory Analysis (EDA)\n\n3.1 Outcome distribution (BMI)\n\n\nSummary statistics: mean, median, SD, quantiles:\nThe response variable, Body Mass Index (BMI), ranged from 15.0 to 81.2 kg/m² with a mean of 28.8 kg/m² (SD = 6.65, median = 27.8).\nApproximately 33% of participants were classified as overweight (25 ≤ BMI &lt; 30) and 33% as obese (BMI ≥ 30), reflecting the high prevalence of excess weight in the U.S. adult population.\nThe distribution exhibited moderate right skewness (skewness = 1.2), indicating a longer tail with high BMI values\n\n\n\n\nDescriptive Statistics for BMI\n\n\nMean\nMedian\nSD\nMin\nMax\nQ1\nQ3\nIQR\nSkewness\n\n\n\n\n28.3\n27.3\n6.2\n15\n67.8\n24\n31.6\n7.6\n1.2\n\n\n\n\n\n\nDistribution of BMI Categories\n\n\nBMI Category\nPatient Count\nPercentage (%)\n\n\n\n\nUnderweight\n35\n1.7\n\n\nNormal\n652\n31.7\n\n\nOverweight\n687\n33.3\n\n\nObese\n686\n33.3\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBMI Distribution plot and outlier detection\n\n\n\n\n\nDistribution plot of BMI (histogram + density curve)\n\n\nThe distribution of the BMI variable is right-skweed as shown in the overview. This shows that most people are around the average BMI in the data; however, some have very high BMI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe box plot indicates that there are no outliners in the lower part ofthe distribution. The lower threshold is at approximately 13, while the upper threshold is approximately at 43.\nIn contrast the upper tail displays a substantial number of extreme values, with 43 observations identified as outliers.\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough the BMI distribution shows high-value observations, these values fall within plausible physiological ranges for the NHANES population. Therefore, no outliers were removed.\n\n\n\n\n\n3.2 Summary of Exploratory Data Analysis BMI vs predictors\nAmong all variables examined, physical activity, race/ethnicity, and education level showed the strongest associations with BMI. Physically active individuals had noticeably lower BMI on average, and several race and education groups displayed meaningful mean differences. In contrast, most continuous predictors such as age, income, sleep hours, blood pressure, and alcohol use—showed very weak correlations and offered limited linear explanatory power. Please see below in the collapsed section for the full EDA.\n\n\n\n\n\n\nNotePairwise relationships with BMI\n\n\n\n\n\n\n3.3 Pairwise relationships with BMI (continuous predictors)\n\nBMI vs Age\n\n\nThe scatterplot with a LOESS smoother shows that BMI remains largely consistent across age groups. The Pearson correlation coefficient (r = 0.0144721) indicates virtually no linear association between age and BMI.\nThe corresponding coefficient of determination (\\(R^{2}\\) = 0.0002094) confirms that age explains less than 0.02% of the variance in BMI. This suggests that BMI is not influenced by age in this sample, and other demographic or lifestyle variables likely play a more substantial role in determining BMI.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs log(income)\n\n\nThe scatterplot with a LOESS smoother shows a weak negative association between BMI and the logarithm of household income.\nThe Pearson correlation coefficient (r = -0.0504988) confirms that higher income is associated with slightly lower BMI values. However, this relationship is very weak (\\(R^{2}\\) = 0.00255), indicating that household income explains less than 1% of the variance in BMI.\nAlthough the direction aligns with the expected negative relationship for higher income, the effect size suggests that income has minimal influence on BMI in this sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs Sleep\n\n\nBMI shows a negligible linear association with sleep duration (r = -0.0321021; \\(R^{2}\\) ≈ 0.001031).\nThe LOESS smoother suggests a shallow U-shape, with the lowest BMI at approximately 7.5 hours of sleep and slightly higher BMI at both shorter and longer durations.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs Systolic Blood Presure\n\n\nAlthough we expected a strong positive relashionship between BMI and systolic blood pressure, the data shows only a very week positive trend. The LOESS curve suggests a small increase in BP with BMI initially, but then the relashioship plateus and even goes down slightly. This indicates that systolic blood presure alone is not a predictor in this sample.\nBMI is expected to predict high blood pressure, but the data may not show this since many patients manage their blood pressure with medication.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI vs AlcoholDay\n\n\nThe LOESS curve is nearly flat with a slight downward tilt. The confidence interval widens as AlcoholDay increases (due to few observations with higher values) The unajusted linear correlation is appox 0.03, meaning very little to no association with BMI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAppendinx: BMI vs Alcohol Day\n\n\n\n\n\nBelow are the BMI–AlcoholDay correlations and visualizations: (i) the plot over the full 0–80 range, and (ii) the log(1 + AlcoholDay)\n\n\nShow code\ncor_alc &lt;- cor(nhanes_lm$AlcoholDay, nhanes_lm$BMI)\ncor_alc\n\n\n[1] 0.03483019\n\n\n(i) the plot over the full 0–80 range\n\n\n\n\n\n\n\n\n\n(ii) the log(1 + AlcoholDay)\n\n\n\n\n\n\n\n\n\n[1] 0.0240489\n\n\nThe correlation between alcohol and BMI is even smaller when AlcoholDay is logaritmized - 0.0240489.\n\n\n\n\n\n3.4 BMI vs Categorical predictors\n\nBMI by Gender\n\n\n \nIn this sample the average for female and male is almost the same; however, the BMI distribution is also more variable among females, as indicated by a higher standard deviation (sd = 7.04 vs. 5.43). There is no evidence that gender plays a strong role in explaining BMI differences in this sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI by Physical Activity\n\n\nOn average, individuals that reported being physically active have a lower BMI (mean ≈ 27.9 kg/\\(m^{2}\\)) than those who are not (mean ≈ 29.9 kg/\\(m^{2}\\)). There is very strong evidence for a difference in BMI between physically active and inactive individuals (p &lt; 2.2 × \\(10^{-16}\\)), with an estimated mean difference of approximately 2.0 units (95% CI: [1.68, 2.36] kg/\\(m^{2}\\)). BMI is also more variable among inactive individuals (SD = 6.9 vs. 5.3), indicating a wider spread of body weight outcomes in this group.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAppendix: Physical Activity Stats and T-test\n\n\n\n\n\n\n\n\nDescriptive Statistics for Physical Active\n\n\nPhysActive\nn\nMean_BMI\nSD_BMI\nMedian_BMI\n\n\n\n\nNo\n989\n29.2\n6.9\n28.0\n\n\nYes\n1071\n27.4\n5.3\n26.6\n\n\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  BMI by PhysActive\nt = 6.6975, df = 1847.8, p-value = 2.805e-11\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 1.285537 2.350209\nsample estimates:\n mean in group No mean in group Yes \n         29.22283          27.40496 \n\n\n\n\n\n\nBMI vs Education\n\n\nIn this sample, individuals who reported having a Colledge Graduate had a lower mean (mean ≈ 27.1 kg/\\(m^{2}\\)) compared with rest of the groups (mean ≈ 28.2 - 29.2 kg/\\(m^{2}\\)). The one-way ANOVA test provides strong evidence that the mean BMI differs across education levels (p &lt; 0.001). However the coefficient of deteermination R2 = 0.0126791 indicates that the education level only explains 1.3% in the BMI variation. This means that, while the difference is statistically significant, its practical importance is very small.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAppendix: Education Stats and Anova and\n\n\n\n\n\n\n\n\nDescriptive Statistics for Education\n\n\nEducation\nn\nMean_BMI\nSD_BMI\nMedian_BMI\n\n\n\n\n8th Grade\n94\n29.2\n6.9\n27.7\n\n\n9 - 11th Grade\n299\n28.9\n7.6\n27.8\n\n\nSome College\n690\n28.7\n6.2\n27.9\n\n\nHigh School\n491\n28.2\n6.0\n27.4\n\n\nCollege Grad\n486\n27.1\n4.9\n26.5\n\n\n\n\n\nAnova\n\n\n              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nEducation      4    990   247.4   6.598 2.84e-05 ***\nResiduals   2055  77068    37.5                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTukey\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = BMI ~ Education, data = nhanes_lm)\n\n$Education\n                                  diff       lwr         upr     p adj\n9 - 11th Grade-8th Grade    -0.2721042 -2.249181  1.70497290 0.9957664\nHigh School-8th Grade       -1.0267140 -2.909063  0.85563478 0.5696549\nSome College-8th Grade      -0.4749704 -2.313186  1.36324503 0.9553039\nCollege Grad-8th Grade      -2.0699378 -3.953842 -0.18603377 0.0229035\nHigh School-9 - 11th Grade  -0.7546099 -1.981100  0.47188025 0.4467253\nSome College-9 - 11th Grade -0.2028662 -1.360483  0.95475066 0.9893126\nCollege Grad-9 - 11th Grade -1.7978337 -3.026709 -0.56895799 0.0006415\nSome College-High School     0.5517436 -0.435414  1.53890127 0.5455991\nCollege Grad-High School    -1.0432238 -2.113055  0.02660738 0.0600524\nCollege Grad-Some College   -1.5949674 -2.585087 -0.60484745 0.0001120\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBMI by Race\n \nIn this sample Black and Mexican groups show higher average BMI than White, while “Other” is lower; the boxplots (red diamonds = means) reflect these shifts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAppendinx: Race Stats and Anova\n\n\n\n\n\n\n\n# A tibble: 5 × 5\n  Race1        n Mean_BMI SD_BMI Median_BMI\n  &lt;fct&gt;    &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Black      187     30.7   8.82       28.6\n2 Mexican    125     30.6   5.72       30.0\n3 Hispanic    83     28.3   4.79       27.1\n4 White     1561     27.9   5.78       27  \n5 Other      104     26.7   5.71       25.2\n\n\nCall:\n   aov(formula = BMI ~ Race1, data = nhanes_lm)\n\nTerms:\n                   Race1 Residuals\nSum of Squares   2263.28  75794.92\nDeg. of Freedom        4      2055\n\nResidual standard error: 6.073152\nEstimated effects may be unbalanced\n\n\nOne-way ANOVA indicates a significant overall difference across Race1 (p &lt; 0.001); pairwise Tukey comparisons can then identify which specific pairs differ.\n\n\n\n\n\n\nBMI vs Smoke\nSmokers show an approximate 1.1 kg/\\(m^{2}\\) lower mean BMI than non-smokers. The T-test result tells us that there is very strong evidence that the median BMI between non-smokers and smokers is not zero.(appendix)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAppendix: BMI~Smoke t-test\n\n\n\n\n\n\n\nShow code\nt_test_Smoke &lt;- t.test(BMI ~ SmokeNow, data = nhanes_lm)\nt_test_Smoke\n\n\n\n    Welch Two Sample t-test\n\ndata:  BMI by SmokeNow\nt = 4.1853, df = 1947.8, p-value = 2.974e-05\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.6065475 1.6762202\nsample estimates:\n mean in group No mean in group Yes \n         28.79577          27.65439",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#linear-model",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#linear-model",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "4 Linear Model",
    "text": "4 Linear Model\nSummary of Linear Modeling Progression (Collapsed Below)\nTo reach the final interaction model, we estimated a sequence of nested linear models. The simple BMI ~ Age regression showed no meaningful association, and adding basic demographics improved the fit only slightly, with race contributing the most. Socioeconomic factors provided minimal additional explanatory power. Lifestyle and clinical variables strengthened the model somewhat, with physical activity, smoking status, systolic blood pressure, and race emerging as the most consistent predictors of BMI. All earlier models and their outputs are collapsed below, while the final interaction model remains visible for interpretation.\n\n\n\n\n\n\nNoteLinear Modeling Progression\n\n\n\n\n\n\n4.1 BMI ~ Age\nWe are starting with a simpler linear model\n\n\nThe intercept is 28.01 kg/\\(m^{2}\\). The interpretation has no meaning as it represents the BMI at age 0 for an adult. (part of the linear fit)\nThe slope is 0.005 kg/\\(m^{2}\\) and the interpretation would be that for the each year increase the BMI will increase with 0.005. The p value is 0.512 and we can say that in this linear model there is no evidence of a linear assocciation between BMI and Age.\nWith a fit \\(R^{2}\\) = 0.0002 (adj. \\(R^{2}\\) = -0.0002) Age explaines none of the variability in BMI\n\n\n\nShow code\nm_age &lt;-lm(BMI ~ Age, data = nhanes_lm)\n\n\n\n\nShow code\n#summary(m_age)\n\n\nage_sum &lt;- tidy(m_age, conf.int = TRUE)\n#age_sum\nage_fit &lt;- glance(m_age)[, c(\"r.squared\",\"adj.r.squared\",\"sigma\",\"nobs\")]\n\n\n\nkable(age_sum, digits=3, caption=\"BMI ~ Age: coefficient table (with 95% CI)\")\n\n\n\nBMI ~ Age: coefficient table (with 95% CI)\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n28.018\n0.418\n67.033\n0.000\n27.198\n28.838\n\n\nAge\n0.005\n0.008\n0.657\n0.512\n-0.011\n0.022\n\n\n\n\n\nShow code\n#kable(age_fit, digits=3, caption=\"BMI ~ Age: model fit\")\nage_fit\n\n\n# A tibble: 1 × 4\n  r.squared adj.r.squared sigma  nobs\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1  0.000209     -0.000276  6.16  2060\n\n\n\n\n\n\n4.2 Model 1: Demographic\nWe will add core demographic variables to the model: Age + Gender + Race\nModel: BMI ~ Age + Gender + Race\n\nWe fit a multiple linear model with BMI as the response and Age (continuous), Gender (female = reference), and Race (White = reference) as covariates.\n\n\n\nShow code\nM1 &lt;- lm(BMI ~ Age + Gender + Race1, data = nhanes_lm)\n\nsummary(M1)\n\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + Race1, data = nhanes_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.210  -4.209  -0.949   3.436  40.186 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   27.117470   0.463686  58.482  &lt; 2e-16 ***\nAge            0.013510   0.008406   1.607   0.1082    \nGendermale     0.221968   0.272635   0.814   0.4156    \nRace1Black     2.872157   0.472655   6.077 1.46e-09 ***\nRace1Mexican   2.802671   0.571678   4.903 1.02e-06 ***\nRace1Other    -1.131178   0.619031  -1.827   0.0678 .  \nRace1Hispanic  0.446401   0.688336   0.649   0.5167    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.071 on 2053 degrees of freedom\nMultiple R-squared:  0.03055,   Adjusted R-squared:  0.02772 \nF-statistic: 10.78 on 6 and 2053 DF,  p-value: 7.701e-12\n\n\nShow code\n# Coefficients with 95% CIs (t-tests)\n# core_coef &lt;- broom::tidy(M1, conf.int = TRUE)\n# core_coef\n\n# M1_sum &lt;- tidy(M1, conf.int = TRUE)\n# kable(M1_sum, digits=3, caption=\"BMI ~ Age + Gender + Race coefficient table (with 95% CI)\")\n\n\n\n# Model fit\n# M1_fit  &lt;- broom::glance(M1)[, c(\"r.squared\",\"adj.r.squared\",\"sigma\",\"df\",\"nobs\")]\n# knitr::kable(M1_fit,  digits = 3, caption = \"Core model: fit statistics\")\n\n\nIn this model the Race differences between groups relative to White: Black (+ 2.87 kg/\\(m^{2}\\), p &lt; \\(10^{-8}\\)) and Mexican (+2.80 kg/\\(m^{2}\\), p &lt; \\(10^{-6}\\)) participants have a higher BMI on average, while Other shows very weak evidence that the BMI is lower on everage than White (-1.13 kg/\\(m^{2}\\), p &lt; 0.068) and for Hispanix group there is no evidence that the BMI is different from White category on average (+0.45 kg/\\(m^{2}\\), p &lt; 0.517). The adjusted \\(R^{2}\\) = 0.028, meaning that demographics only explain ~2.8% of BMI variability\nF-tests for M1 (BMI ~ Age + Gender + Race1)\n\n\nShow code\nM1_drop1 &lt;- drop1(M1, test = \"F\")\nM1_drop1 \n\n\nSingle term deletions\n\nModel:\nBMI ~ Age + Gender + Race1\n       Df Sum of Sq   RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;              75673 7437.7                      \nAge     1     95.20 75768 7438.3  2.5827    0.1082    \nGender  1     24.43 75698 7436.3  0.6629    0.4156    \nRace1   4   2318.52 77992 7491.8 15.7252 1.109e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nUsing drop1() we test each term conditional on the others. Race is associated with BMI; Age and Gender are not, at this stage.\n\n\n4.3 Adding Socioeconomic factors\nWe are adding socioeconomic factors to our model\n\n\nShow code\nM2 &lt;- update(M1, . ~ . + Education + log_income)\nsummary(M2)\n\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + Race1 + Education + log_income, \n    data = nhanes_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.858  -4.169  -0.913   3.454  39.780 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             28.034963   2.066514  13.566  &lt; 2e-16 ***\nAge                      0.015941   0.008473   1.881   0.0601 .  \nGendermale               0.191993   0.272588   0.704   0.4813    \nRace1Black               2.671400   0.481595   5.547 3.28e-08 ***\nRace1Mexican             2.649685   0.593333   4.466 8.41e-06 ***\nRace1Other              -1.077031   0.623601  -1.727   0.0843 .  \nRace1Hispanic            0.344172   0.692346   0.497   0.6192    \nEducation9 - 11th Grade -0.030781   0.733418  -0.042   0.9665    \nEducationHigh School    -0.621045   0.703421  -0.883   0.3774    \nEducationSome College    0.164729   0.698290   0.236   0.8135    \nEducationCollege Grad   -1.224686   0.723380  -1.693   0.0906 .  \nlog_income              -0.055827   0.185911  -0.300   0.7640    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.053 on 2048 degrees of freedom\nMultiple R-squared:  0.03873,   Adjusted R-squared:  0.03357 \nF-statistic: 7.501 on 11 and 2048 DF,  p-value: 9.159e-13\n\n\nAfteradding the Education and log_income as covariates the BMI remains higher forBlack and Mexican participants vs White. Age and particpats that are Colledge Graduates shows a very weak association with BMI. The adj. \\(R^{2}\\) shows an explanability of 3.4%\nTerm-wise F-tests\n\n\nShow code\nM2_drop1 &lt;- drop1(M2, test = \"F\")\nM2_drop1\n\n\nSingle term deletions\n\nModel:\nBMI ~ Age + Gender + Race1 + Education + log_income\n           Df Sum of Sq   RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                  75035 7430.2                      \nAge         1    129.68 75165 7431.8  3.5393  0.060071 .  \nGender      1     18.18 75053 7428.7  0.4961  0.481305    \nRace1       4   1912.91 76948 7474.1 13.0527 1.687e-10 ***\nEducation   4    599.27 75634 7438.6  4.0891  0.002643 ** \nlog_income  1      3.30 75038 7428.3  0.0902  0.763989    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTerm-wise F-tests: BMI differs overall by Race and Education; Age is borderline; Gender and log(Income) show little added association (conditional on other covariates).\n\n\n4.4 Adding Lifestyle & Clinical predictors\n\n\nShow code\nM3 &lt;- update(M2, . ~ . + PhysActive + SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve)\nsummary(M3)\n\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + Race1 + Education + log_income + \n    PhysActive + SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve, \n    data = nhanes_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.915  -3.999  -0.831   3.537  37.748 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             29.573516   2.335666  12.662  &lt; 2e-16 ***\nAge                     -0.015609   0.009670  -1.614 0.106648    \nGendermale               0.014294   0.274346   0.052 0.958454    \nRace1Black               3.006900   0.476562   6.310 3.42e-10 ***\nRace1Mexican             2.241999   0.584376   3.837 0.000129 ***\nRace1Other              -0.531417   0.616321  -0.862 0.388656    \nRace1Hispanic            0.416809   0.679956   0.613 0.539948    \nEducation9 - 11th Grade  0.089781   0.720029   0.125 0.900781    \nEducationHigh School    -0.516741   0.690459  -0.748 0.454304    \nEducationSome College    0.253163   0.685239   0.369 0.711830    \nEducationCollege Grad   -0.926147   0.719078  -1.288 0.197904    \nlog_income              -0.110284   0.183536  -0.601 0.547982    \nPhysActiveYes           -1.809196   0.278270  -6.502 9.96e-11 ***\nSleepHrsNight           -0.082643   0.098545  -0.839 0.401770    \nAlcoholDay               0.074228   0.041594   1.785 0.074476 .  \nSmokeNowYes             -2.181678   0.299600  -7.282 4.67e-13 ***\nBPSysAve                 0.022322   0.008524   2.619 0.008893 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.928 on 2043 degrees of freedom\nMultiple R-squared:  0.08013,   Adjusted R-squared:  0.07293 \nF-statistic: 11.12 on 16 and 2043 DF,  p-value: &lt; 2.2e-16\n\n\nAfter adding lifestyle and clinical predictors to our Model BMI we can observe that BMI stays higher for Black and Mexican in comparision with White participants. Physically Active participants have a lower BMI, and Smoking participants have a lower BMI. Effects are conditional on the others (treatment coding); results are associations, not causal.\nTerm-wise F-tests\n\n\nShow code\nM3_drop1 &lt;- drop1(M3, test = \"F\")\nM3_drop1\n\n\nSingle term deletions\n\nModel:\nBMI ~ Age + Gender + Race1 + Education + log_income + PhysActive + \n    SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve\n              Df Sum of Sq   RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                     71803 7349.5                      \nAge            1     91.57 71895 7350.2  2.6055  0.106648    \nGender         1      0.10 71804 7347.5  0.0027  0.958454    \nRace1          4   1852.00 73655 7394.0 13.1736 1.346e-10 ***\nEducation      4    430.36 72234 7353.8  3.0612  0.015826 *  \nlog_income     1     12.69 71816 7347.9  0.3611  0.547982    \nPhysActive     1   1485.64 73289 7389.7 42.2705 9.962e-11 ***\nSleepHrsNight  1     24.72 71828 7348.2  0.7033  0.401770    \nAlcoholDay     1    111.93 71915 7350.7  3.1848  0.074476 .  \nSmokeNow       1   1863.69 73667 7400.3 53.0269 4.673e-13 ***\nBPSysAve       1    241.01 72044 7354.4  6.8573  0.008893 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTerm-wise F-tests for M3 (BMI ~ Age + Gender + Race1 + Education + log_income + PhysActive + SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve)\n\n\n\n\n\n4.5 Adding Interactions\nWe extended the model with prespecified interactions to test whether the association between:\n\nPhysActive × Gender - Based on Gender Differences in Exercise Habits and Quality of Life Reports1, physical activity patterns differ significantly by gender. Here we test whether the BMI–activity association varies by sex.\nPhysActive × Education - According to the research Education leads to a more physically active lifestyle2, “one additional year of education leads to a 0.62-unit higher overall physical activity”. We are testing if in our sample the activity–BMI association varies across socioeconomic factors (education levels).\nGender × SmokeNow - Based on the report from Swiss association for tabacco control3, there are known gender differences in smoking paterns. In out sample we are testing whether the smoking-BMI association differs by sex.\n\n\nM4 &lt;- update(M3, . ~ . + PhysActive:Gender + PhysActive:Education + Gender:SmokeNow)\n\n\n\n\n\n\n\nNoteModel Summary- result\n\n\n\n\n\n\n\nShow code\nsummary(M4)\n\n\n\nCall:\nlm(formula = BMI ~ Age + Gender + Race1 + Education + log_income + \n    PhysActive + SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve + \n    Gender:PhysActive + Education:PhysActive + Gender:SmokeNow, \n    data = nhanes_lm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -4.057  -0.895   3.396  37.770 \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                           27.726849   2.380324  11.648  &lt; 2e-16 ***\nAge                                   -0.014841   0.009598  -1.546  0.12221    \nGendermale                             0.611595   0.482437   1.268  0.20504    \nRace1Black                             2.890277   0.473409   6.105 1.23e-09 ***\nRace1Mexican                           2.292266   0.580298   3.950 8.08e-05 ***\nRace1Other                            -0.520097   0.611616  -0.850  0.39522    \nRace1Hispanic                          0.585686   0.679784   0.862  0.38902    \nEducation9 - 11th Grade                1.154768   0.876821   1.317  0.18799    \nEducationHigh School                  -0.145821   0.849861  -0.172  0.86378    \nEducationSome College                  0.855728   0.839405   1.019  0.30811    \nEducationCollege Grad                  0.269230   0.937998   0.287  0.77412    \nlog_income                            -0.065774   0.183019  -0.359  0.71935    \nPhysActiveYes                         -0.262527   1.337241  -0.196  0.84438    \nSleepHrsNight                         -0.099740   0.098131  -1.016  0.30956    \nAlcoholDay                             0.084933   0.041469   2.048  0.04068 *  \nSmokeNowYes                           -0.661655   0.433203  -1.527  0.12683    \nBPSysAve                               0.025625   0.008505   3.013  0.00262 ** \nGendermale:PhysActiveYes               1.084032   0.538843   2.012  0.04437 *  \nEducation9 - 11th Grade:PhysActiveYes -3.183134   1.482250  -2.148  0.03187 *  \nEducationHigh School:PhysActiveYes    -1.531381   1.409680  -1.086  0.27746    \nEducationSome College:PhysActiveYes   -2.013071   1.376690  -1.462  0.14383    \nEducationCollege Grad:PhysActiveYes   -2.849886   1.435170  -1.986  0.04720 *  \nGendermale:SmokeNowYes                -2.650774   0.538496  -4.923 9.23e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.879 on 2037 degrees of freedom\nMultiple R-squared:  0.09791,   Adjusted R-squared:  0.08817 \nF-statistic: 10.05 on 22 and 2037 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nInteraction observation:\n\nGender:SmokingNow: there is very strong evidence that smoking is linked to a lower BMI for both sexes: women ~ 0.7 kg/\\(m^{2}\\) lower and men ~ 3.3 kg/\\(m^{2}\\) compared with non-smokers.\nPhysActivity:Gender: there is evidence that difference in average BMIassociated with physical acivity in not 0 men. In the baseline group women show a small decrease with activity (~ 0.3kg/\\(m^{2}\\)). Men add approx 1.1 kg/\\(m^{2}\\) to the female difference.\nPhysActivity:Education: in lower (9–11) and higher (College) education groups, being active is associated with a noticeably lower BMI than in the 8th-grade group.\n\n\nAll of these interaction are conditional associations (not causal)\n\nAs we added the interaction we oberve that AlcoholDay shows a small positive association with BMI (for each additional drink a day the BMI increases with 0.085 kg/\\(m^{2}\\), p = 0.04), conditional on other covariates.\nTerm-wise F-tests\n\nM4_drop1 &lt;- drop1(M4, test = \"F\")\n\n\n\n\n\n\n\nNoteF-test result\n\n\n\n\n\n\n\nShow code\nM4_drop1\n\n\nSingle term deletions\n\nModel:\nBMI ~ Age + Gender + Race1 + Education + log_income + PhysActive + \n    SleepHrsNight + AlcoholDay + SmokeNow + BPSysAve + Gender:PhysActive + \n    Education:PhysActive + Gender:SmokeNow\n                     Df Sum of Sq   RSS    AIC F value    Pr(&gt;F)    \n&lt;none&gt;                            70416 7321.3                      \nAge                   1     82.64 70498 7321.7  2.3907   0.12221    \nRace1                 4   1759.80 72175 7364.2 12.7270 3.116e-10 ***\nlog_income            1      4.46 70420 7319.5  0.1292   0.71935    \nSleepHrsNight         1     35.71 70451 7320.4  1.0330   0.30956    \nAlcoholDay            1    145.00 70561 7323.6  4.1947   0.04068 *  \nBPSysAve              1    313.77 70729 7328.5  9.0769   0.00262 ** \nGender:PhysActive     1    139.91 70556 7323.4  4.0473   0.04437 *  \nEducation:PhysActive  4    263.23 70679 7321.0  1.9037   0.10719    \nGender:SmokeNow       1    837.64 71253 7343.7 24.2315 9.229e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nTerm-wise F-tests summary: BMI is associated with race, systolic BP, and shows effect modification for Gender:Smoking and Gender:Physical activity. Education:Physical activity is not supported",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#research-questions",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#research-questions",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "5 Research Questions",
    "text": "5 Research Questions\nAfter adjusting for covariates, how does BMI vary with Age?\nAdjusted for all covariates, the term-wise F-test (drop1) shows little evidence of a linear association between age and BMI (p ≈ 0.12)\nDo demographic factors show overall association with BMI?\n\nRace/ethnicity: Yes. Strong overall association (clear F-test).\nEducation: Yes (overall main effect), but no activity–education interaction.\nGender: No large main effect, but gender modifies the associations of physical activity and smoking with BMI.\n\nAre lifestile factors (PsyActive, AlcoholDay, SleepNight, SmokeNow) associated with BMI, and how much?\n\nAt the reference education (8th Grade), females show a small reduction with activity (PhysActive main term).\nMales add the Gender:PhysActive interaction, show an increase with activity.\nIn 9–11th Grade and College Grad, the Education:PhysActive interactions are negative, showing that there is an associated reduction than in the reference education. However with the Drop1 test there is weak to no evidence that there is an interaction bewteen Education and PhyActive\n\nHow much variance is explained by the model?\nThe model’s adjusted \\(R^{2}\\) ≈ 0.088, so the model explains only 8.8% of the variability in BMI.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/01_linear_model.html#footnotes",
    "href": "ml-portfolio/02_nhanes_R/01_linear_model.html#footnotes",
    "title": "Linear Regression Analysis - Predicting Body Mass Index",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee PMC article.↩︎\nSee PMC article.↩︎\nSee AT report.↩︎",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project",
      "Linear Regression Analysis - Predicting Body Mass Index"
    ]
  },
  {
    "objectID": "ml-portfolio/02_nhanes_R/index.html",
    "href": "ml-portfolio/02_nhanes_R/index.html",
    "title": "NHANES Project (R)",
    "section": "",
    "text": "This section will host the NHANES health analysis project, including regression models, classification, and data exploration.\nContent will be added soon.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "NHANES - R Project"
    ]
  },
  {
    "objectID": "ml-portfolio/index.html",
    "href": "ml-portfolio/index.html",
    "title": "Machine Learning Portfolio",
    "section": "",
    "text": "This section presents machine learning work developed in R and Python.\nProjects included:\n\nR Labs (example: SVM on iris)\nNHANES health analysis in R (regression + SVM)\nDiabetes readmission prediction in Python (end-to-end pipeline)\n\nMore projects will be added as the MSc program continues.",
    "crumbs": [
      "Home",
      "ML Portfolio",
      "Overview"
    ]
  },
  {
    "objectID": "standalone-projects/index.html",
    "href": "standalone-projects/index.html",
    "title": "Standalone Projects",
    "section": "",
    "text": "This section includes analytical projects that are not strictly machine learning models but still demonstrate data science and applied research skills.\nCurrent projects:\n\nBitcoin & Portfolio Analysis (R)\nSwiss Energy Production and Solar Radiation (R)",
    "crumbs": [
      "Home",
      "Standalone Projects",
      "Overview"
    ]
  }
]